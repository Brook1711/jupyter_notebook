{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tensorflow 强化学习入门"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用环境：\n",
    "\n",
    "ubuntu-20.04LTS\n",
    "\n",
    "cuda 11.1\n",
    "\n",
    "nvidia 460.73\n",
    "\n",
    "python 3.7.10\n",
    "\n",
    "tensorflow 2.5.0rc1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n",
      "version of tensorflow is :   2.5.0-rc1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf_V = tf.__version__\n",
    "print(\"version of tensorflow is :  \", tf_V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**检验是否可以使用GPU**\n",
    "\n",
    "test if the gpu is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-17bb7203622b>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有些时候nvidia会自动更新显卡驱动并导致bug，此时会报错：“Failed to initialize NVML: Driver/library version mismatc”， 并且此时在tensorflow中无法使用GPU（上述操作输出为“False”）\n",
    "\n",
    "解决方案：sudo reboot （重启电脑）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensorflow 教程资源"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf 自2.4版本之后在api中提供了直接调用DQN、DDPG等网络的函数，十分方便\n",
    "\n",
    "tensorflow官方也放出了有关教程：\n",
    "\n",
    "1、所有教程\n",
    "https://tensorflow.google.cn/tutorials\n",
    "\n",
    "2.1、actor-critic架构\n",
    "https://tensorflow.google.cn/tutorials/reinforcement_learning/actor_critic\n",
    "    相关notebook 下载：\n",
    "    https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/reinforcement_learning/actor_critic.ipynb\n",
    "    目前已经在该目录下\n",
    "    \n",
    "2.2、tf-agent 用法\n",
    "https://tensorflow.google.cn/agents\n",
    "    相关notebook 下载：\n",
    "    https://github.com/tensorflow/agents\n",
    "    目前已经在本目录下\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 强化学习简介\n",
    "> 引用自tensorflow强化学习官方文档 0_intro_rl\n",
    "\n",
    "> Reinforcement learning (RL) is a general framework where agents learn to perform actions in an environment so as to maximize a reward. The two main components are the environment, which represents the problem to be solved, and the agent, which represents the learning algorithm.\n",
    "\n",
    "强化学习（Reinforcement learning，RL）是一个通用框架，在该框架中智能体（agent）可以与环境通过输出动作（action）进行交互并最大化奖励（reward）。其中我们可以看到，在这个框架中一共有两个主要组成部分：1、环境（environment），即智能体要解决的问题。2、智能体（agent），即解决问题的学习算法。\n",
    "\n",
    "> The agent and environment continuously interact with each other. At each time step, the agent takes an action on the environment based on its *policy* $\\pi(a_t|s_t)$, where $s_t$ is the current observation from the environment, and receives a reward $r_{t+1}$ and the next observation $s_{t+1}$ from the environment. The goal is to improve the policy so as to maximize the sum of rewards (return).\n",
    "\n",
    "智能体和环境之间会持续地相互交互。在训练中的每一个步骤（step）中，智能体都会通过**策略（*policy*）（$\\pi(a_t|s_t)$**生成一个对应的动作（action）（$a_t$），其中$s_t$是当前从环境观测到的状态（observation），并且智能体在每一次和环境的交互之后都会接收到一个奖励（reward）（$r_{t+1}$）和下一个步骤的状态（$s{t+1}$）。总体的目标是最大化奖励（reward）\n",
    "\n",
    "> Note: It is important to distinguish between the `state` of the environment and the `observation`, which is the part of the environment `state` that the agent can see, e.g. in a poker game, the environment state consists of the cards belonging to all the players and the community cards, but the agent can observe only its own cards and a few community cards. In most literature, these terms are used interchangeably and observation is also denoted as $s$.\n",
    "\n",
    "需要注意的是：需要区分状态（state）和观测（observation）这两个概念。观测（observation）是指整个环境里面所有的状态，状态（state）只是观测（observation）的一部分，即智能体所能接触到的信息。比如说在一个扑克游戏里。state指的是agent只能看得到的自己的纸牌，observation是所有玩家的扑克牌\n",
    "\n",
    "> This is a very general framework and can model a variety of sequential decision making problems such as games, robotics etc. \n",
    "\n",
    "强化学习是一个泛化性很强的框架，可以适用于各种连续决策问题（sequential decision making problems），例如游戏、机器人等等。\n",
    "\n",
    "## 举例：倒立摆问题（CartPole）\n",
    "> The Cartpole environment is one of the most well known classic reinforcement learning problems ( the *\"Hello, World!\"* of RL). A pole is attached to a cart, which can move along a frictionless track. The pole starts upright and the goal is to prevent it from falling over by controlling the cart.  \n",
    "\n",
    "倒立摆问题（CartPole）相当于是强化学习中的“hello world”问题。在该问题中，一根杆子连接到一个只能沿着一条直线移动到滑块上。杆子在初始时刻是垂直的，需要调整滑块的运动来保证杆子不再倒下。\n",
    "\n",
    "> *   The observation from the environment $s_t$ is a 4D vector representing the position and velocity of the cart, and the angle and angular velocity of the pole. \n",
    "> *   The agent can control the system by taking one of 2 actions $a_t$: push the cart right (+1) or left (-1). \n",
    "> *   A reward $r_{t+1} = 1$ is provided for every timestep that the pole remains upright. The episode ends when one of the following is true:\n",
    ">  * the pole tips over some angle limit\n",
    ">  * the cart moves outside of the world edges\n",
    ">  * 200 time steps pass. \n",
    "\n",
    "*  对环境的观测（$s_t$）是一个四维矢量，其中分别是滑块的位置和速度以及杆子的角度和角速度。\n",
    "*  智能体的动作（$a_t$）只有一个维度，而且取离散值（+1，-1），其中+1代表向右推动滑块；-1代表向左推动滑块  The agent can control the system by taking one of 2 actions $a_t$: push the cart right (+1) or left (-1). \n",
    "*  如果在第$t$个时刻（step），杆子依然没有倒下，那么就给一个正的reward：$r_{t+1}=1$\n",
    "  * the pole tips over some angle limit\n",
    "  * the cart moves outside of the world edges\n",
    "  * 200 time steps pass. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-2.5.0",
   "language": "python",
   "name": "tf-2.5.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
